{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "intro",
			"metadata": {},
			"source": [
				"# ðŸ¦† Level 0: The Playground (In-Memory)\n",
				"\n",
				"Welcome to the **DuckDB Laboratory**. This notebook is for rapid experimentation and testing relational logic without touching the disk.\n",
				"\n",
				"### Why this matters:\n",
				"- **Zero Setup**: No database files to manage.\n",
				"- **Lazy Power**: Queries aren't run until you call `.show()` or `.df()`.\n",
				"- **Variable Querying**: You can use Python variables as if they were SQL tables!"
			]
		},
		{
			"cell_type": "markdown",
			"id": "step1-header",
			"metadata": {},
			"source": [
				"## ðŸ—ï¸ 1. Setup & In-Memory Tables\n",
				"We start by creating our e-commerce mini-world in RAM."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "step1-code",
			"metadata": {},
			"outputs": [],
			"source": [
				"import duckdb\n",
				"\n",
				"# Create the 'users' table in memory\n",
				"duckdb.sql(\"\"\"\n",
				"CREATE OR REPLACE TABLE users (\n",
				"    user_id INTEGER,\n",
				"    name VARCHAR,\n",
				"    join_date DATE\n",
				");\n",
				"\n",
				"INSERT INTO users VALUES\n",
				"    (1, 'Alice', '2023-01-01'),\n",
				"    (2, 'Bob', '2023-02-15'),\n",
				"    (3, 'Charlie', '2023-03-10'),\n",
				"    (4, 'Diana', '2023-04-20');\n",
				"\"\"\")\n",
				"\n",
				"# Create the 'orders' table in memory\n",
				"duckdb.sql(\"\"\"\n",
				"CREATE OR REPLACE TABLE orders (\n",
				"    order_id INTEGER,\n",
				"    user_id INTEGER,\n",
				"    amount DECIMAL(10, 2)\n",
				");\n",
				"\n",
				"INSERT INTO orders VALUES\n",
				"    (101, 1, 300.50),\n",
				"    (102, 1, 250.00),\n",
				"    (103, 2, 50.00),\n",
				"    (104, 3, 600.00),\n",
				"    (105, 4, 100.00),\n",
				"    (106, 4, 150.00);\n",
				"\"\"\")\n",
				"\n",
				"print(\"âœ… Success: 'users' and 'orders' created in-memory.\")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "step2-header",
			"metadata": {},
			"source": [
				"## ðŸ‘‘ 2. Case Study: VIP Hunter\n",
				"We will join our tables to find customers who have spent more than $500."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "step2-code",
			"metadata": {},
			"outputs": [],
			"source": [
				"# The SQL way to hunt VIPs\n",
				"vip_rel = duckdb.sql(\"\"\"\n",
				"    SELECT \n",
				"        u.name,\n",
				"        SUM(o.amount) AS total_spend\n",
				"    FROM users u\n",
				"    JOIN orders o ON u.user_id = o.user_id\n",
				"    GROUP BY u.name\n",
				"    HAVING total_spend > 500\n",
				"    ORDER BY total_spend DESC\n",
				"\"\"\")\n",
				"\n",
				"print(\"VIPs found!\")\n",
				"vip_rel.show()"
			]
		},
		{
			"cell_type": "markdown",
			"id": "step3-header",
			"metadata": {},
			"source": [
				"## ðŸš„ 3. Zero-Copy Bridge (DuckDB -> Arrow)\n",
				"This is the 'Universal Language' of data. We move data to **Arrow** without any overhead (Zero-Copy)."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "step3-code",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Materialize the users table into an Arrow Table\n",
				"arrow_table = duckdb.sql(\"SELECT * FROM users\").to_arrow_table()\n",
				"\n",
				"print(\"---------------------------------------------------\")\n",
				"print(f\"Data Type: {type(arrow_table)}\")\n",
				"print(\"---------------------------------------------------\")\n",
				"print(f\"Schema:\\n{arrow_table.schema}\")\n",
				"print(\"---------------------------------------------------\")\n",
				"print(f\"Arrow Table Contents:\\n{arrow_table}\")\n",
				"print(\"---------------------------------------------------\")\n",
				"\n",
				"# Bonus: Convert back to Pandas for pretty Jupyter layout\n",
				"arrow_table.to_pandas()"
			]
		},
		{
			"cell_type": "markdown",
			"id": "step3.5-header",
			"metadata": {},
			"source": [
				"## ðŸš€ 4. The Speed of Polars (Level 2 Preview)\n",
				"Since Polars speaks Arrow natively, we can move our DuckDB data into a **Polars DataFrame** for high-speed analysis."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "step3.5-code",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Materialize directly to Polars (Zero-Copy)\n",
				"polars_df = duckdb.sql(\"SELECT * FROM users\").pl()\n",
				"\n",
				"print(\"---------------------------------------------------\")\n",
				"print(f\"Data Type: {type(polars_df)}\")\n",
				"print(\"---------------------------------------------------\")\n",
				"print(f\"Polars Schema:\\n{polars_df.schema}\")\n",
				"print(\"---------------------------------------------------\")\n",
				"print(f\"Polars DataFrame:\\n{polars_df}\")\n",
				"print(\"---------------------------------------------------\")\n",
				"\n",
				"# Bonus: Convert back to Pandas for pretty Jupyter layout\n",
				"polars_df.to_pandas()"
			]
		},
		{
			"cell_type": "markdown",
			"id": "step4-header",
			"metadata": {},
			"source": [
				"## ðŸ’¾ 5. Persistent Export (Arrow -> Parquet)\n",
				"Finally, we save our Arrow table to a physical Parquet file using the standard `pyarrow` library."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "step4-code",
			"metadata": {},
			"outputs": [],
			"source": [
				"import pyarrow.parquet as pq\n",
				"import os\n",
				"\n",
				"# Ensure the warehouse directory exists (relative to /notebooks/)\n",
				"os.makedirs('../data/warehouse', exist_ok=True)\n",
				"\n",
				"# Save to disk\n",
				"pq.write_table(arrow_table, '../data/warehouse/arrow_export.parquet')\n",
				"\n",
				"print(\"ðŸŽ Filesystem Updated: data/warehouse/arrow_export.parquet is now ready for production!\")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "knowledge-base",
			"metadata": {},
			"source": [
				"# ðŸ§  Knowledge Base: Common Questions\n",
				"\n",
				"### Q1: What other options can we convert SQL tables to?\n",
				"DuckDB is a \"Universal Translator.\" You can convert results into almost any major format:\n",
				"| Method | Target | Use Case |\n",
				"| :--- | :--- | :--- |\n",
				"| **`.df()`** | **Pandas** | The \"Old Reliable.\" Best for small data. |\n",
				"| **`.pl()`** | **Polars** | The \"Speed Demon.\" Much faster than Pandas. |\n",
				"| **`.fetchnumpy()`** | **NumPy** | Best for Machine Learning (tensors/arrays). |\n",
				"| **`.fetchall()`** | **Python List** | Best for simple app logic. |\n",
				"| **`.to_arrow_table()`** | **Arrow** | Best for interoperability. |\n",
				"\n",
				"### Q2: `arrow_table.to_pandas()` â€” What is it and what else?\n",
				"Arrow is **Memory-only**. Itâ€™s just a \"Binary Block\" of data. To interact with it, you convert it:\n",
				"- **`.to_pandas()`**: Converts Arrow to a Pandas DataFrame (very fast).\n",
				"- **`pl.from_arrow(arrow_table)`**: Move it into **Polars** (Zero-Copy king).\n",
				"- **`arrow_table.to_pylist()`**: Convert to a standard Python list.\n",
				"- **`arrow_table.to_batches()`**: Break it into small \"chunks\" for streaming.\n",
				"\n",
				"### Q3: Why use Arrow to save to Parquet instead of just DuckDB?\n",
				"Technically, DuckDB can save to Parquet directly: duckdb.sql(\"COPY table TO 'file.parquet'\")\n",
				"\n",
				"SO WHY USE ARROW?\n",
				"1. **Engine Independence**: Arrow is a universal language. You can swap engines later without changing your save logic.\n",
				"2. **Streaming**: Arrow is designed to \"pipe\" data between systems (e.g., Database to Web API).\n",
				"3. **Metadata Control**: PyArrow gives you surgical control over Parquet settings (compression, etc.).\n",
				"4. **Zero-Copy**: Arrow is a zero-copy format, meaning it can be read by multiple systems without copying the data.\n",
				"ðŸŽ“ The \"Golden Rule\" of this Stack:\n",
				"Use DuckDB to query/crunch the data, but use Arrow to pass that data around. This is why we call it the \"Zero-Copy\" stackâ€”Arrow is the bridge that keeps everything fast.\n",
				"\n",
				"### Q4: Is Zero-Copy lost when using Polars (Level 2)?\n",
				"**No!** Polars uses Apache Arrow as its internal memory format. When you call `.pl()`, DuckDB places data into an Arrow block, and Polars simply points its \"eyes\" at it. No copying happens.\n",
				"```\n",
				"graph LR\n",
				"    A[DuckDB Engine] -- \".pl()\" --> B(Arrow Memory Block)\n",
				"    B -- \"Zero Copy\" --> C[Polars DataFrame]\n",
				"\n",
				"```\n",
				"DuckDB: Queries the data.\n",
				"The Transfer: When you call .pl() in DuckDB, it doesn't \"copy\" the data into Polars. Instead, it places the data into an Arrow Memory Block.\n",
				"Polars: Points its own \"eyes\" at that exact same memory block.\n",
				"\n",
				"ðŸ§ª Proof you can see in your code:\n",
				"```python\n",
				"# This one line handles the whole \"Zero-Copy\" bridge automatically\n",
				"polars_df = duckdb.sql(\"SELECT * FROM users\").pl()\n",
				"```\n",
				"\n",
				"ðŸ§  Why we use Polars instead of raw Arrow?\n",
				"Raw Arrow Table: Great for storage and moving data, but very hard to code with. (Try doing a GROUP BY in raw Arrowâ€”it's painful!)\n",
				"Polars: Gives you a beautiful, easy API (like Pandas) to do complex data science, while still using the speed of Arrow underneath.\n",
				"Summary:\n",
				"You aren't losing the Zero-Copy logic in Level 2; you are harnessing it.\n",
				"\n",
				"Arrow is the Track.\n",
				"Polars is the High-Speed Train running on that track.\n",
				"\n",
				"### Q5: What is the best practice for writing Parquet: `arrow_table` or `polars_df`?\n",
				"| Method | Best For... | Why? |\n",
				"| :--- | :--- | :--- |\n",
				"| **`polars_df.write_parquet()`** | **Data Science & Logic** | Fast, multithreaded (Rust), and convenient. |\n",
				"| **`pq.write_table(arrow_table)`** | **Infrastructure & Transport** | Maximum control over metadata and compatibility. |\n",
				"\n",
				"1. Polars (polars_df.write_parquet())\n",
				"Best For: When you are doing logic (filtering, joining, calculating).\n",
				"The Difference: It uses a high-performance Rust-based writer. It is incredibly fast for local files because it is multithreaded by default.\n",
				"Practice: If the data is already in a Polars DataFrame, always use df.write_parquet().\n",
				"2. PyArrow (pq.write_table(arrow_table))\n",
				"Best For: Pure infrastructure and \"handing off\" data.\n",
				"The Difference: It is the \"Industry Standard.\" It gives you surgical control. You can set specific Parquet versions, custom encryption, or compression levels (like Zstd level 22).\n",
				"Practice: Use this when you are building a \"bridge\" between two systems (like DuckDB and a Cloud Bucket) and don't need to do any Python-based math on the data.\n",
				"\n",
				"### Q6: What exactly is the \"Arrow Bridge\" (The Analogy)?\n",
				"Think of your data flow like **Water**:\n",
				"\n",
				"The arrow_table you created is the actual \"Bridge\" (the data itself), and pyarrow (pq) is the construction worker that uses that bridge to build a file.\n",
				"\n",
				"Here is the breakdown of the \"Bridge\" in your code:\n",
				"\n",
				"1. The Bridge (The Data Format)\n",
				"Object: arrow_table\n",
				"Role: This is the data sitting in your RAM. It is no longer \"owned\" by DuckDB. It is now in a universal format that Polars, Pandas, and Spark all understand perfectly.\n",
				"Why we call it a bridge: Because it allows data to move between different software engines without being \"re-written\" or \"re-formatted\" (Zero-Copy).\n",
				"2. The Worker (The Tool)\n",
				"Object: import pyarrow.parquet as pq\n",
				"Role: This is just the library (the \"construction tool\") that has the instructions for how to take an Arrow Table and save it as a Parquet file on your hard drive.\n",
				"ðŸŽ¨ The Analogy:\n",
				"Think of your data like Water:\n",
				"\n",
				"DuckDB is a high-tech Pressure Tank. It's great for swirling the water around (SQL), but it's hard for other people to use that specific tank.\n",
				"arrow_table is a Standard Bucket. You've poured the water out of the tank and into a bucket that everyone in the neighborhood recognizes.\n",
				"pyarrow (pq) is a Funnel. Itâ€™s the tool you use to pour that bucket into a Glass Bottle (the Parquet file) so you can keep it in the fridge forever.\n",
				"Summary:\n",
				"DuckDB = The Engine (Querying).\n",
				"arrow_table = The Bridge (The Memory-Format)."
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": ".venv",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.12.0"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
